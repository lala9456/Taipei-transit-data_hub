{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import Conflict, NotFound\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_bq_from_gcs(\n",
    "    client: bigquery.Client,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    "    bucket_name: str,\n",
    "    blob_name: str,\n",
    "    schema: List[bigquery.SchemaField] = None,\n",
    "    filetype: str = \"parquet\",\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Build a bigquery external table from a file in GCS.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use to create the external table.\n",
    "        dataset_name (str): The name of the dataset to create.\n",
    "        table_name (str): The name of the table to create.\n",
    "        bucket_name (str): The name of the bucket to upload to.\n",
    "        blob_name (str): The name of the blob to upload to.\n",
    "        schema (List[bigquery.SchemaField], optional): \n",
    "        The schema of the table to upload to. Default is None.\n",
    "        If None, use the default schema (automatic-detect).\n",
    "        filetype (str): The type of the file to download. Default is \"parquet\". \n",
    "        Can be \"parquet\" or \"csv\" or \"jsonl\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the upload was successful, False otherwise.\n",
    "    \"\"\"  # noqa\n",
    "    # Construct the fully-qualified BigQuery table ID\n",
    "    table_id = f\"{client.project}.{dataset_name}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_id)  # Attempt to get the table\n",
    "        print(f\"Table {table_id} already exists.\")\n",
    "        return False\n",
    "    except NotFound:\n",
    "        # Define the external data source configuration\n",
    "        if filetype == \"parquet\":\n",
    "            external_config = bigquery.ExternalConfig(\"PARQUET\")\n",
    "        elif filetype == \"csv\":\n",
    "            external_config = bigquery.ExternalConfig(\"CSV\")\n",
    "        elif filetype == \"jsonl\":\n",
    "            external_config = bigquery.ExternalConfig(\"JSONL\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid filetype: {filetype}. Please specify 'parquet' or 'csv' or 'jsonl'.\"\n",
    "            )\n",
    "        external_config.source_uris = [f\"gs://{bucket_name}/{blob_name}\"]\n",
    "        if schema:\n",
    "            external_config.schema = schema\n",
    "        # Create a table with the external data source configuration\n",
    "        table = bigquery.Table(table_id)\n",
    "        table.external_data_configuration = external_config\n",
    "\n",
    "        try:\n",
    "            # API request to create the external table\n",
    "            client.create_table(table)\n",
    "            print(f\"External table {table.table_id} created.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to create external table, reason: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"An error occurred while checking if the table exists: {e}\")\n",
    "\n",
    "\n",
    "def query_bq(client: bigquery.Client, sql_query: str) -> bigquery.QueryJob:\n",
    "    \"\"\"\n",
    "    Query bigquery and return results. (可以用在bigquery指令，例如Insert、Update，但沒有要取得資料表的資料)\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use to query bigquery.\n",
    "        sql_query (str): The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.QueryJob: The result of the query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = client.query(sql_query)\n",
    "        return query_job.result()  # Return the results for further processing\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to query bigquery table, reason: {e}\")\n",
    "\n",
    "\n",
    "def query_bq_to_df(client: bigquery.Client, sql_query: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes a BigQuery SQL query and directly loads the results into a DataFrame\n",
    "    using the BigQuery Storage API.  (可以用在bigquery指令，然後取得資料表的資料成為DataFrame)\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use to query bigquery.\n",
    "        query (str): SQL query string.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The query results as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = client.query(sql_query)\n",
    "        return query_job.to_dataframe()  # Convert result to DataFrame\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to query bigquery table, reason: {e}\")\n",
    "\n",
    "\n",
    "def upload_df_to_bq(\n",
    "    client: bigquery.Client,\n",
    "    df: pd.DataFrame,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    "    schema: List[bigquery.SchemaField] = None,\n",
    "    filetype: str = \"parquet\",\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Upload a pandas dataframe to bigquery.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use to upload to bigquery.\n",
    "        df (pd.DataFrame): The dataframe to upload.\n",
    "        dataset_name (str): The name of the dataset to upload to.\n",
    "        table_name (str): The name of the table to upload to.\n",
    "        schema (List[bigquery.SchemaField], optional): The schema of the table to upload to. Default is None.\n",
    "                                                        If None, use the default schema (automatic-detect).\n",
    "        filetype (str): The type of the file to download. Default is \"parquet\". Can be \"parquet\" or \"csv\" or \"jsonl\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the upload was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    dataset_id = client.dataset(dataset_name)\n",
    "    table_id = dataset_id.table(table_name)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    )\n",
    "    if filetype == \"parquet\":\n",
    "        job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "    elif filetype == \"csv\":\n",
    "        job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    elif filetype == \"jsonl\":\n",
    "        job_config.source_format = bigquery.SourceFormat.JSONL\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid filetype: {filetype}. Please specify 'parquet' or 'csv' or 'jsonl'.\"\n",
    "        )\n",
    "    if schema:\n",
    "        job_config.schema = schema\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        table = client.get_table(table_id)\n",
    "        print(f\"Table {table.table_id} created with {table.num_rows} rows.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to upload df to bigquery, reason: {e}\")\n",
    "\n",
    "\n",
    "def delete_table(client: bigquery.Client, dataset_name: str, table_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a bigquery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use to delete the table.\n",
    "        dataset_name (str): The name of the dataset to delete the table from.\n",
    "        table_name (str): The name of the table to delete.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the deletion was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    table_id = f\"{dataset_name}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"Table {table_id} deleted.\")\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_id} not found.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def rename_table(\n",
    "    client: bigquery.Client, dataset_name: str, table_name: str, new_table_name: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Rename a BigQuery table by creating a new table with the new name and copying data from the old table,\n",
    "    then deleting the old table. Handles both regular and external tables.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): The client to use with BigQuery.\n",
    "        dataset_name (str): The name of the dataset containing the table.\n",
    "        table_name (str): The current name of the table.\n",
    "        new_table_name (str): The new name for the table.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the rename was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    dataset_ref = client.dataset(dataset_name)\n",
    "    old_table_ref = dataset_ref.table(table_name)\n",
    "    new_table_ref = dataset_ref.table(new_table_name)\n",
    "\n",
    "    try:\n",
    "        old_table = client.get_table(old_table_ref)\n",
    "\n",
    "        if old_table.table_type == \"EXTERNAL\":\n",
    "            # Create a new external table with the same configuration\n",
    "            new_table = bigquery.Table(new_table_ref, schema=old_table.schema)\n",
    "            new_table.external_data_configuration = (\n",
    "                old_table.external_data_configuration\n",
    "            )\n",
    "            client.create_table(new_table)\n",
    "            print(f\"External table {table_name} renamed to {new_table_name}.\")\n",
    "\n",
    "            # Optionally delete the old external table\n",
    "            client.delete_table(old_table_ref)\n",
    "            print(f\"Old external table {table_name} deleted.\")\n",
    "        else:\n",
    "            # Handle regular tables\n",
    "            new_table = bigquery.Table(new_table_ref, schema=old_table.schema)\n",
    "            new_table.time_partitioning = old_table.time_partitioning\n",
    "            new_table.range_partitioning = old_table.range_partitioning\n",
    "            new_table.clustering_fields = old_table.clustering_fields\n",
    "            new_table.description = old_table.description\n",
    "            client.create_table(new_table)\n",
    "\n",
    "            # Copy data from the old table to the new table\n",
    "            job = client.copy_table(old_table_ref, new_table_ref)\n",
    "            job.result()  # Wait for the job to complete\n",
    "\n",
    "            # Delete the old table\n",
    "            client.delete_table(old_table_ref)\n",
    "            print(f\"Table {table_name} renamed to {new_table_name}.\")\n",
    "\n",
    "        return True\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_name} not found.\")\n",
    "        return False\n",
    "    except Conflict:\n",
    "        print(f\"Conflict occurred: Table {new_table_name} already exists.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to rename table, reason: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGQUERY_CREDENTIALS_FILE_PATH = r\"D:\\data_engineer\\TIR_group2\\TIR101_Group2\\secrets\\harry_GCS_BigQuery_write_cred.json\"\n",
    "BIGQUERY_CREDENTIALS_FILE_PATH = r\"D:\\data_engineer\\dev_TIR_group2\\Taipei-transit-data_hub\\airflow\\dags\\andy-gcs_key.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = BIGQUERY_CREDENTIALS_FILE_PATH\n",
    "BQ_CLIENT = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ETL_FACT.FACT_youbike_mrt_distance deleted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_table(client=BQ_CLIENT, dataset_name=\"ETL_FACT\", table_name=\"FACT_youbike_mrt_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ETL_FACT.DIM_youbike_mrt_distance deleted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_table(client=BQ_CLIENT, dataset_name=\"ETL_FACT\", table_name=\"DIM_youbike_mrt_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
